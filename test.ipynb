{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading movie-corpus to C:\\Users\\11632\\.convokit\\downloads\\movie-corpus\n",
      "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, download\n",
    "corpus = Corpus(filename=download(\"movie-corpus\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总话语数: 304713\n",
      "总对话数（会话数）: 83097\n",
      "对话ID: L1044\n",
      "u0: They do not!\n",
      "u2: They do to!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"总话语数:\", len(corpus.utterances))\n",
    "print(\"总对话数（会话数）:\", len(corpus.conversations))\n",
    "\n",
    "\n",
    "for conversation_id in corpus.conversations:\n",
    "    conversation = corpus.get_conversation(conversation_id)\n",
    "    print(f\"对话ID: {conversation_id}\")\n",
    "    for utterance in conversation.iter_utterances():\n",
    "        print(f\"{utterance.speaker.id}: {utterance.text}\")\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_texts = []\n",
    "\n",
    "# \n",
    "for i, conversation_id in enumerate(corpus.conversations):\n",
    "    if i >= 10000:  \n",
    "        break\n",
    "    conversation = corpus.get_conversation(conversation_id)\n",
    "   \n",
    "    conversation_text = ' '.join([utterance.text for utterance in conversation.iter_utterances()])\n",
    "    conversations_texts.append(conversation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "# create a tokenizer\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502498\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = encoding.encode(\"\".join(conversations_texts))\n",
    "\n",
    "print(len(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([502498])\n",
      "100252\n"
     ]
    }
   ],
   "source": [
    "#convert to tensor\n",
    "tokenized_text = torch.tensor(tokenized_text)\n",
    "print(tokenized_text.shape)\n",
    "max_token_value = tokenized_text.max().item()\n",
    "print(max_token_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and validation sets\n",
    "train_idex = int(len(tokenized_text) * 0.9)\n",
    "train_data = tokenized_text[:train_idex]\n",
    "valid_data = tokenized_text[train_idex:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "context_size = 64\n",
    "d_model = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly extract a batch of data from train data\n",
    "data = train_data\n",
    "idxs = torch.randint(0 , len(data) - context_size, size = (batch_size,))\n",
    "x_batch = torch.stack([data[idx:idx + context_size] for idx in idxs])\n",
    "y_batch = torch.stack([data[idx + 1 :idx + context_size + 1] for idx in idxs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457</td>\n",
       "      <td>13</td>\n",
       "      <td>9930</td>\n",
       "      <td>4359</td>\n",
       "      <td>369</td>\n",
       "      <td>279</td>\n",
       "      <td>15326</td>\n",
       "      <td>1274</td>\n",
       "      <td>13</td>\n",
       "      <td>33739</td>\n",
       "      <td>...</td>\n",
       "      <td>690</td>\n",
       "      <td>584</td>\n",
       "      <td>30</td>\n",
       "      <td>2675</td>\n",
       "      <td>323</td>\n",
       "      <td>264</td>\n",
       "      <td>2763</td>\n",
       "      <td>315</td>\n",
       "      <td>1023</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>584</td>\n",
       "      <td>3358</td>\n",
       "      <td>1440</td>\n",
       "      <td>1405</td>\n",
       "      <td>311</td>\n",
       "      <td>5662</td>\n",
       "      <td>499</td>\n",
       "      <td>13</td>\n",
       "      <td>3639</td>\n",
       "      <td>...</td>\n",
       "      <td>430</td>\n",
       "      <td>596</td>\n",
       "      <td>837</td>\n",
       "      <td>11</td>\n",
       "      <td>719</td>\n",
       "      <td>358</td>\n",
       "      <td>20781</td>\n",
       "      <td>1781</td>\n",
       "      <td>1131</td>\n",
       "      <td>10445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4726</td>\n",
       "      <td>24532</td>\n",
       "      <td>279</td>\n",
       "      <td>4398</td>\n",
       "      <td>315</td>\n",
       "      <td>1057</td>\n",
       "      <td>5961</td>\n",
       "      <td>1345</td>\n",
       "      <td>5544</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>18083</td>\n",
       "      <td>13</td>\n",
       "      <td>2722</td>\n",
       "      <td>300</td>\n",
       "      <td>35283</td>\n",
       "      <td>11</td>\n",
       "      <td>369</td>\n",
       "      <td>499</td>\n",
       "      <td>1198</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11649</td>\n",
       "      <td>11</td>\n",
       "      <td>433</td>\n",
       "      <td>596</td>\n",
       "      <td>6740</td>\n",
       "      <td>3814</td>\n",
       "      <td>323</td>\n",
       "      <td>499</td>\n",
       "      <td>2351</td>\n",
       "      <td>264</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>656</td>\n",
       "      <td>499</td>\n",
       "      <td>617</td>\n",
       "      <td>264</td>\n",
       "      <td>2457</td>\n",
       "      <td>18396</td>\n",
       "      <td>30</td>\n",
       "      <td>358</td>\n",
       "      <td>42210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>292</td>\n",
       "      <td>73302</td>\n",
       "      <td>789</td>\n",
       "      <td>83</td>\n",
       "      <td>4208</td>\n",
       "      <td>1466</td>\n",
       "      <td>0</td>\n",
       "      <td>20524</td>\n",
       "      <td>1037</td>\n",
       "      <td>9608</td>\n",
       "      <td>...</td>\n",
       "      <td>33621</td>\n",
       "      <td>13</td>\n",
       "      <td>220</td>\n",
       "      <td>1102</td>\n",
       "      <td>596</td>\n",
       "      <td>7742</td>\n",
       "      <td>13</td>\n",
       "      <td>23371</td>\n",
       "      <td>13</td>\n",
       "      <td>4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1283</td>\n",
       "      <td>5097</td>\n",
       "      <td>264</td>\n",
       "      <td>3828</td>\n",
       "      <td>2085</td>\n",
       "      <td>264</td>\n",
       "      <td>50169</td>\n",
       "      <td>1198</td>\n",
       "      <td>264</td>\n",
       "      <td>3828</td>\n",
       "      <td>...</td>\n",
       "      <td>358</td>\n",
       "      <td>1781</td>\n",
       "      <td>499</td>\n",
       "      <td>527</td>\n",
       "      <td>264</td>\n",
       "      <td>58725</td>\n",
       "      <td>430</td>\n",
       "      <td>596</td>\n",
       "      <td>4461</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>433</td>\n",
       "      <td>13</td>\n",
       "      <td>3639</td>\n",
       "      <td>30</td>\n",
       "      <td>423</td>\n",
       "      <td>3988</td>\n",
       "      <td>10555</td>\n",
       "      <td>13</td>\n",
       "      <td>4800</td>\n",
       "      <td>358</td>\n",
       "      <td>...</td>\n",
       "      <td>2771</td>\n",
       "      <td>499</td>\n",
       "      <td>1524</td>\n",
       "      <td>19937</td>\n",
       "      <td>433</td>\n",
       "      <td>30</td>\n",
       "      <td>88383</td>\n",
       "      <td>433</td>\n",
       "      <td>13</td>\n",
       "      <td>3639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>323</td>\n",
       "      <td>44202</td>\n",
       "      <td>315</td>\n",
       "      <td>18157</td>\n",
       "      <td>13</td>\n",
       "      <td>1283</td>\n",
       "      <td>374</td>\n",
       "      <td>264</td>\n",
       "      <td>16888</td>\n",
       "      <td>47228</td>\n",
       "      <td>...</td>\n",
       "      <td>315</td>\n",
       "      <td>54242</td>\n",
       "      <td>13</td>\n",
       "      <td>578</td>\n",
       "      <td>89662</td>\n",
       "      <td>374</td>\n",
       "      <td>11604</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>3314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1     2      3     4     5      6      7      8      9   ...  \\\n",
       "0   1457     13  9930   4359   369   279  15326   1274     13  33739  ...   \n",
       "1     11    584  3358   1440  1405   311   5662    499     13   3639  ...   \n",
       "2   4726  24532   279   4398   315  1057   5961   1345   5544     13  ...   \n",
       "3  11649     11   433    596  6740  3814    323    499   2351    264  ...   \n",
       "4    292  73302   789     83  4208  1466      0  20524   1037   9608  ...   \n",
       "5   1283   5097   264   3828  2085   264  50169   1198    264   3828  ...   \n",
       "6    433     13  3639     30   423  3988  10555     13   4800    358  ...   \n",
       "7    323  44202   315  18157    13  1283    374    264  16888  47228  ...   \n",
       "\n",
       "      54     55    56     57     58     59     60     61    62     63  \n",
       "0    690    584    30   2675    323    264   2763    315  1023   1274  \n",
       "1    430    596   837     11    719    358  20781   1781  1131  10445  \n",
       "2  18083     13  2722    300  35283     11    369    499  1198    358  \n",
       "3     11    656   499    617    264   2457  18396     30   358  42210  \n",
       "4  33621     13   220   1102    596   7742     13  23371    13   4800  \n",
       "5    358   1781   499    527    264  58725    430    596  4461    311  \n",
       "6   2771    499  1524  19937    433     30  88383    433    13   3639  \n",
       "7    315  54242    13    578  89662    374  11604      0   578   3314  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", we'll know where to reach you. What can we do to assist you?Regardless of what you think, Lieutenant, the fact remains that David is missing and that we must find him. The forensic lads seem to feel that some sort of animal was involved, that's true, but I hardly think...Why\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode(x_batch[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.8084,  0.0639, -0.3472,  ..., -0.2894,  0.4802, -1.0599],\n",
      "        [ 0.2934, -0.7432, -0.6012,  ..., -0.2541,  0.6344, -0.4346],\n",
      "        [-0.6278,  0.0846, -0.4237,  ...,  1.2919,  1.1441, -0.1380],\n",
      "        ...,\n",
      "        [ 1.0076,  0.8602, -0.5991,  ..., -0.2053, -1.3240, -1.1955],\n",
      "        [ 0.7903,  0.4072,  1.2660,  ..., -0.7030, -0.7543,  0.9546],\n",
      "        [ 1.3568, -1.9735,  2.4950,  ..., -0.8447, -0.1116, -0.4855]],\n",
      "       requires_grad=True)\n",
      "torch.Size([8, 64, 64])\n",
      "torch.Size([8, 64, 64])\n",
      "Parameter containing:\n",
      "tensor([[-0.7015,  0.1402, -0.6608,  ..., -0.0070,  0.4238, -1.3672],\n",
      "        [-0.2809,  0.0829,  0.6477,  ...,  1.4171,  0.8764,  0.0279],\n",
      "        [-0.8238,  0.9602,  0.6292,  ..., -1.3885,  0.3483, -1.1689],\n",
      "        ...,\n",
      "        [-0.3615, -0.7396, -0.7769,  ..., -0.3373, -0.5814, -0.6948],\n",
      "        [-0.4642,  0.1719, -0.9179,  ...,  0.1603,  0.1119, -1.2467],\n",
      "        [-0.2487, -0.0665,  0.7068,  ...,  0.5214, -0.8784,  0.4992]],\n",
      "       requires_grad=True)\n",
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "#embedding 层\n",
    "''' \n",
    "embedding层的作用是将单词嵌入为语义向量，它的输入是模型的输入X。输出单词的语义信息。\n",
    "\n",
    "在gpt使用的Transformer中，语义分为两种，一是单词本身语义，二是单词所处位置的语义。\n",
    "\n",
    "换句话说，上次的预测结果提供两种信息\n",
    "\n",
    "1.词语是什么？\n",
    "2.词语的位置是什么？\n",
    "\n",
    "'''\n",
    "\n",
    "token_embedding_table = torch.nn.Embedding(max_token_value + 1, d_model)\n",
    "#打印embedding层的权重\n",
    "print(token_embedding_table.weight)\n",
    "x_batch_embedding = token_embedding_table(x_batch)\n",
    "y_batch_embedding = token_embedding_table(y_batch)\n",
    "print(x_batch_embedding.shape)\n",
    "print(y_batch_embedding.shape)\n",
    "\n",
    "#形状： X,T,C\n",
    "#X: batch_size 批次大小\n",
    "#T: context_size 上下文大小，序列长度，时间步\n",
    "#C: d_model 词向量维度\n",
    "\n",
    "'''\n",
    "position = torch.arange(0, context_size, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "position_encoding = torch.zeros(context_size, d_model)\n",
    "position_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "position_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "position_encoding = position_encoding.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "x = x_batch_embedding + position_encoding\n",
    "y = y_batch_embedding + position_encoding\n",
    "'''\n",
    "\n",
    "'''\n",
    "预先计算位置编码的值（而不是使用可训练的嵌入）的主要优点是我们的模型最终需要训练的参数更少。参数的减少可以提高训练性能\n",
    "'''\n",
    "#获取位置编码\n",
    "position_encoding = torch.nn.Embedding(context_size, d_model)\n",
    "print(position_encoding.weight)\n",
    "print(position_encoding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 64, 64]) torch.Size([8, 64, 64]) torch.Size([8, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "#multihead attention\n",
    "'''\n",
    "batch_size = 8\n",
    "context_size = 64\n",
    "d_model = 64\n",
    "'''\n",
    "num_heads = 4\n",
    "head_dim = d_model // num_heads  # 每个头的维度\n",
    "\n",
    "# 64 * 64\n",
    "Wq = torch.nn.Linear(d_model, d_model)\n",
    "Wk = torch.nn.Linear(d_model, d_model)\n",
    "Wv = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "Q = Wq(x_batch_embedding)\n",
    "K = Wk(x_batch_embedding)\n",
    "V = Wv(x_batch_embedding)\n",
    "\n",
    "print(Q.shape, K.shape, V.shape)\n",
    "\n",
    "# 将Q, K, V按照多头设置进行维度重排\n",
    "Q_multihead = Q.view(batch_size, context_size, num_heads, head_dim)\n",
    "Q_multihead = Q_multihead.transpose(1, 2)  # 将头的维度和上下文长度的维度交换\n",
    "\n",
    "K_multihead = K.view(batch_size, context_size, num_heads, head_dim)\n",
    "K_multihead = K_multihead.transpose(1, 2)\n",
    "\n",
    "V_multihead = V.view(batch_size, context_size, num_heads, head_dim)\n",
    "V_multihead = V_multihead.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = Q @ K.transpose(-2 , -1)\n",
    "wei.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.187471</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.004797</td>\n",
       "      <td>-2.234551</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.993447</td>\n",
       "      <td>-0.542453</td>\n",
       "      <td>1.525857</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.496064</td>\n",
       "      <td>0.938212</td>\n",
       "      <td>-1.086502</td>\n",
       "      <td>-3.696027</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.345954</td>\n",
       "      <td>0.483479</td>\n",
       "      <td>-0.616461</td>\n",
       "      <td>-5.035115</td>\n",
       "      <td>-0.393587</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>...</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-2.508794</td>\n",
       "      <td>0.534034</td>\n",
       "      <td>2.798499</td>\n",
       "      <td>7.071962</td>\n",
       "      <td>1.866140</td>\n",
       "      <td>-2.058599</td>\n",
       "      <td>0.825363</td>\n",
       "      <td>0.200114</td>\n",
       "      <td>0.534034</td>\n",
       "      <td>1.829050</td>\n",
       "      <td>...</td>\n",
       "      <td>3.226939</td>\n",
       "      <td>-4.524257</td>\n",
       "      <td>1.118625</td>\n",
       "      <td>-2.542966</td>\n",
       "      <td>4.873235</td>\n",
       "      <td>-1.918636</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4.301505</td>\n",
       "      <td>0.961278</td>\n",
       "      <td>0.726928</td>\n",
       "      <td>-0.021345</td>\n",
       "      <td>-2.444457</td>\n",
       "      <td>-0.957070</td>\n",
       "      <td>3.757560</td>\n",
       "      <td>0.268312</td>\n",
       "      <td>0.961278</td>\n",
       "      <td>2.591392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139885</td>\n",
       "      <td>1.461952</td>\n",
       "      <td>-2.637837</td>\n",
       "      <td>1.624761</td>\n",
       "      <td>1.352815</td>\n",
       "      <td>-0.004836</td>\n",
       "      <td>0.457659</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-4.246055</td>\n",
       "      <td>-0.543369</td>\n",
       "      <td>-1.452324</td>\n",
       "      <td>3.743972</td>\n",
       "      <td>3.100425</td>\n",
       "      <td>-2.824816</td>\n",
       "      <td>0.717740</td>\n",
       "      <td>-0.435698</td>\n",
       "      <td>-0.543369</td>\n",
       "      <td>-0.682876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634515</td>\n",
       "      <td>-3.148952</td>\n",
       "      <td>-0.557993</td>\n",
       "      <td>-0.391002</td>\n",
       "      <td>2.907906</td>\n",
       "      <td>-1.497793</td>\n",
       "      <td>1.820893</td>\n",
       "      <td>-3.245041</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>-1.104063</td>\n",
       "      <td>1.111376</td>\n",
       "      <td>-2.176154</td>\n",
       "      <td>-3.728298</td>\n",
       "      <td>-6.003815</td>\n",
       "      <td>-1.022707</td>\n",
       "      <td>-1.283503</td>\n",
       "      <td>-1.320779</td>\n",
       "      <td>1.111376</td>\n",
       "      <td>3.099248</td>\n",
       "      <td>...</td>\n",
       "      <td>2.576442</td>\n",
       "      <td>-2.168800</td>\n",
       "      <td>-0.747793</td>\n",
       "      <td>-3.900452</td>\n",
       "      <td>-1.248537</td>\n",
       "      <td>5.893105</td>\n",
       "      <td>4.446800</td>\n",
       "      <td>-0.172687</td>\n",
       "      <td>6.724002</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1.117253</td>\n",
       "      <td>-0.530971</td>\n",
       "      <td>1.355114</td>\n",
       "      <td>2.199233</td>\n",
       "      <td>1.061515</td>\n",
       "      <td>-0.291648</td>\n",
       "      <td>-0.031272</td>\n",
       "      <td>2.570066</td>\n",
       "      <td>-0.530971</td>\n",
       "      <td>-0.217701</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.016725</td>\n",
       "      <td>-2.766339</td>\n",
       "      <td>3.155781</td>\n",
       "      <td>0.888438</td>\n",
       "      <td>1.896731</td>\n",
       "      <td>0.502376</td>\n",
       "      <td>0.458164</td>\n",
       "      <td>3.075670</td>\n",
       "      <td>-0.262940</td>\n",
       "      <td>2.570066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   3.187471      -inf      -inf      -inf      -inf      -inf      -inf   \n",
       "1  -2.004797 -2.234551      -inf      -inf      -inf      -inf      -inf   \n",
       "2   0.993447 -0.542453  1.525857      -inf      -inf      -inf      -inf   \n",
       "3  -3.496064  0.938212 -1.086502 -3.696027      -inf      -inf      -inf   \n",
       "4   1.345954  0.483479 -0.616461 -5.035115 -0.393587      -inf      -inf   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "59 -2.508794  0.534034  2.798499  7.071962  1.866140 -2.058599  0.825363   \n",
       "60  4.301505  0.961278  0.726928 -0.021345 -2.444457 -0.957070  3.757560   \n",
       "61 -4.246055 -0.543369 -1.452324  3.743972  3.100425 -2.824816  0.717740   \n",
       "62 -1.104063  1.111376 -2.176154 -3.728298 -6.003815 -1.022707 -1.283503   \n",
       "63  1.117253 -0.530971  1.355114  2.199233  1.061515 -0.291648 -0.031272   \n",
       "\n",
       "          7         8         9   ...        54        55        56        57  \\\n",
       "0       -inf      -inf      -inf  ...      -inf      -inf      -inf      -inf   \n",
       "1       -inf      -inf      -inf  ...      -inf      -inf      -inf      -inf   \n",
       "2       -inf      -inf      -inf  ...      -inf      -inf      -inf      -inf   \n",
       "3       -inf      -inf      -inf  ...      -inf      -inf      -inf      -inf   \n",
       "4       -inf      -inf      -inf  ...      -inf      -inf      -inf      -inf   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "59  0.200114  0.534034  1.829050  ...  3.226939 -4.524257  1.118625 -2.542966   \n",
       "60  0.268312  0.961278  2.591392  ...  0.139885  1.461952 -2.637837  1.624761   \n",
       "61 -0.435698 -0.543369 -0.682876  ...  0.634515 -3.148952 -0.557993 -0.391002   \n",
       "62 -1.320779  1.111376  3.099248  ...  2.576442 -2.168800 -0.747793 -3.900452   \n",
       "63  2.570066 -0.530971 -0.217701  ... -1.016725 -2.766339  3.155781  0.888438   \n",
       "\n",
       "          58        59        60        61        62        63  \n",
       "0       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "1       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "2       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "3       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "4       -inf      -inf      -inf      -inf      -inf      -inf  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "59  4.873235 -1.918636      -inf      -inf      -inf      -inf  \n",
       "60  1.352815 -0.004836  0.457659      -inf      -inf      -inf  \n",
       "61  2.907906 -1.497793  1.820893 -3.245041      -inf      -inf  \n",
       "62 -1.248537  5.893105  4.446800 -0.172687  6.724002      -inf  \n",
       "63  1.896731  0.502376  0.458164  3.075670 -0.262940  2.570066  \n",
       "\n",
       "[64 rows x 64 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply mask\n",
    "mask = torch.triu(torch.ones(context_size, context_size), diagonal=1).bool()\n",
    "wei = wei.masked_fill(mask, float('-inf'))\n",
    "pd.DataFrame(wei[0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#softmax\n",
    "attention_score = torch.nn.Softmax(dim=-1)(wei)\n",
    "\n",
    "#计算多头注意力\n",
    "attention = attention_score @ V\n",
    "\n",
    "#将多头注意力拼接\n",
    "output = attention.transpose(1, 2).contiguous().view(batch_size, context_size, d_model)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply residual connection\n",
    "output = attention + x_batch_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply layer normalization\n",
    "layer_norm = torch.nn.LayerNorm(d_model)\n",
    "output = layer_norm(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a feedforward network\n",
    "'''\n",
    "feedforward层由两个全连接层组成，这两个全连接层之间有一个ReLU激活函数。\n",
    "'''\n",
    "feedforward = torch.nn.Sequential(\n",
    "    torch.nn.Linear(d_model, 4 * d_model),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4 * d_model, d_model)\n",
    ")\n",
    "output = feedforward(output)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply next layer normalization\n",
    "output = layer_norm(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 100253])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply final layer linear transformation\n",
    "output = torch.nn.Linear(d_model, max_token_value + 1)(output)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' mov'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = F.softmax(output, dim=-1)\n",
    "predicted_index =  torch.argmax(logits[0,0]).item()\n",
    "encoding.decode([predicted_index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CUDA",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
